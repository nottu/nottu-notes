\documentclass{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{listings}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{multicol}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{helvet}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[table,xcdraw]{xcolor}

\decimalpoint
\graphicspath{ {./} }

\title {Tarea 4, Reconocimiento de Patrones}
  \author {Francisco Javier Peralta Ramírez}
  \date{\vspace{-2ex}}

\begin{document}
\vspace{-2ex}
\maketitle
\begin{enumerate}

\item Calcula el clasificador bayesiano optimo para Y. Podemos calcular $P(X), P(Y)$ y usamos los datos de la tabla, los cuales muestran los valores para $P(X \cap Y)$

\begin{multicols}{3}
\small
  \begin{align*}
    P(X = 0) &= \cup_{y = 0}^{2} P( X = 0 \cap Y = y)\\
    P(X = 0) &= P(X = 0 \cap Y = 0) \\&+ P(X = 0 \cap Y = 1)\\
    P(X = 0) &= 0.2 + 0.15 = 0.35
  \end{align*}\break
  \begin{align*}
    P(X = 1) &= \cup_{y = 0}^{2} P( X = 1 \cap Y = y)\\
    P(X = 1) &= P(X = 1 \cap Y = 0) \\&+ P(X = 1 \cap Y = 1)\\
    P(X = 1) &= 0.15 + 0.1 = 0.25
  \end{align*}\break
  \begin{align*}
    P(X = 2) &= \cup_{y = 0}^{2} P( X = 2 \cap Y = y)\\
    P(X = 2) &= P(X = 2 \cap Y = 0) \\&+ P(X = 2 \cap Y = 1)\\
    P(X = 2) &= 0.3 + 0.1 = 0.4
  \end{align*}
\end{multicols}

Usando la definición de las propiedades condicionales tenemos que $P(Y = y | X = x) = P(X = x \cap Y = x) / P(X)$ con lo que obtenemos

\begin{multicols}{3}
\small
  \begin{align*}
    P(Y = 1 | X = 0) &= \frac{P( X = 0 \cap Y = 1)}{P(X = 0)}\\
                     &= \frac{0.2}{0.35} = 0.57\\
    P(Y = 2 | X = 0) &= \frac{P( X = 0 \cap Y = 2)}{P(X = 0)}\\
                     &= \frac{0.15}{0.35} = 0.43\\
         \rightarrow & \text{Tomamos } Y = 1
  \end{align*}\break
  \begin{align*}
    P(Y = 1 | X = 1) &= \frac{P( X = 1 \cap Y = 1)}{P(X = 1)}\\
                     &= \frac{0.15}{0.25} = 0.6\\
    P(Y = 2 | X = 1) &= \frac{P( X = 1 \cap Y = 2)}{P(X = 1)}\\
                     &= \frac{0.1}{0.25} = 0.4\\
         \rightarrow & \text{Tomamos } Y = 1
  \end{align*}\break
  \begin{align*}
    P(Y = 1 | X = 2) &= \frac{P( X = 2 \cap Y = 1)}{P(X = 2)}\\
                     &= \frac{0.3}{0.4} = 0.75\\
    P(Y = 2 | X = 2) &= \frac{P( X = 2 \cap Y = 2)}{P(X = 2)}\\
                     &= \frac{0.1}{0.4} = 0.25\\
         \rightarrow & \text{Tomamos } Y = 1
  \end{align*}
\end{multicols}

\item Deriva el clasificador Bayesiano Óptimo para el caso de dos clases y una funció de costo simétrica cuando:
  $$ P(X = x|Y = 1) \sim \mathcal{N}(\mu_1, \Sigma) \qquad P(X = x|Y = 2) \sim \mathcal{N}(\mu_2, \Sigma) $$
y
  $$ P(Y = 1) = 2 P(Y = 2)$$

La regla de Bayes nos dice:
  $$P(Y = y | X = x) = \frac{P(X = x | Y = y)}{P(X)}P(Y = y)$$

Ya que la función de costo es simétrica, tomamos $Y = 1$ si $P(Y = 1 | X = x) > P(Y = 2 | X = x)$ y $Y = 2$ de lo contrario. Con esto tenemos la desigualdad

  \begin{align}
    P(Y = 1 | X = x)                      &> P(Y = 2 | X = x)\\
    \frac{P(X = x | Y = 1)}{P(X)}P(Y = 1) &> \frac{P(X = x | Y = 2)}{P(X)}P(Y = 2)\\
    P(X = x | Y = 1)P(Y = 1)              &> P(X = x | Y = 2)P(Y = 2)\\
    2P(X = x | Y = 1)                     &> P(X = x | Y = 2)\\
    2 \mathcal{N}(\mu_1, \Sigma)          &> \mathcal{N}(\mu_2, \Sigma)\\
    2\mu_1 + 2\mathcal{N}(0, \Sigma)      &> \mathcal{N}(0, \Sigma) + \mu_2\\
    \mathcal{N}(0, \Sigma)                &> \mu_2 - 2\mu_1
  \end{align}

Podemos calcular $P(X)$ usando %$P(X = x) = P(X=x | Y = 1)P(Y = 1) + P(X=x | Y = 2)P(Y = 2)$

  \begin{align*}
    P(X = x) &= P(X=x | Y = 1)P(Y = 1) + P(X=x | Y = 2)P(Y = 2)\\
    P(X = x) &= \mathcal{N}(\mu_1, \Sigma)P(Y = 1) + \mathcal{N}(\mu_2, \Sigma)P(Y = 2)\\
    P(X = x) &= \mathcal{N}(\mu_1, \Sigma)\frac{2}{3} + \mathcal{N}(\mu_2, \Sigma)\frac{1}{3}\\
    P(X = x) &= \frac{2\mu_1}{3} + \mathcal{N}(0, \Sigma)\frac{2}{3} + \frac{\mu_2}{3} + \mathcal{N}(0, \Sigma)\frac{1}{3}\\
    P(X = x) &= \frac{1}{3}(2\mu_1 + \mu_2) + \mathcal{N}(0, \Sigma)\\
    X &\sim \mathcal{N}(\frac{1}{3}(2\mu_1 + \mu_2), \Sigma)
  \end{align*}

En la ecuación \emph{7} podemos sumar $\frac{1}{3}(2\mu_1 + \mu_2)$ a ambos lados

\begin{align*}
\mathcal{N}(0, \Sigma) + \frac{1}{3}(2\mu_1 + \mu_2) &> \mu_2 - 2\mu_1 + \frac{1}{3}(2\mu_1 + \mu_2)\\
X &> \mu_2 - 2\mu_1 + \frac{1}{3}(2\mu_1 + \mu_2)\\
X &> \frac{1}{3}(3\mu_2 - 6\mu_1 + 2\mu_1 + \mu_2)\\
X &> \frac{4}{3}(\mu_2 -\mu_1)\\
\end{align*}

En otras palabras, escogemos $Y = 1$ cuando $X > \frac{4}{3}(\mu_2 -\mu_1)$

\item Considera un problema de clasificación de $X$ en dos categorías, $Y = 0$ y $ Y = 1$. Supongamos que $P(Y = 0) = P(Y = 1)$ y que $P(X | Y = i)$ sigue una distribución Poisson con parámetro $\lambda_i$.

Derive el clasificador Bayesiano óptimo si el costo de un falso positivo es dos veces el costo de un falso negativo.
\end{enumerate}
\end{document}