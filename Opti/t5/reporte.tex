\documentclass[10pt,twocolumn]{article}
\usepackage[margin=0.5in]{geometry}
\usepackage{listings}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\decimalpoint
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{helvet}
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\familydefault}{\sfdefault}
\graphicspath{ {./} }
\usepackage{multirow}
\usepackage[table,xcdraw]{xcolor}

\title {Optimización Tarea 5}
  \author {Francisco Javier Peralta Ramírez}

\begin{document}
\maketitle
\begin{enumerate}

\item Implementar busqueda de linea con máximo descenso usando los métodos de \emph{backtracking}, \emph{interpolación cuadrática} y \emph{cúbica}. Aplica las implementaciones a las siguientes funciones y compara los resultados con respecto a : el número de interaciones, la norma del gradiente $||\nabla f(\boldsymbol{x}_k)||$ y el error $|f(\boldsymbol{x}_k) - f(\boldsymbol{x}^*)|$

\begin{enumerate}
\item Función de Rosembrock, para n = 2 y n = 100
\vspace{-2ex}
\begin{align*}
f(\boldsymbol{x}) &= [100 (x_2 - x_1^2)^2 + (1 - x_1)^2] \\
\boldsymbol{x^0} &= [-1.2, 1]^T \\
\boldsymbol{x^*} &= [1, 1]^T \\
f(\boldsymbol{x^*}) &= 0
\end{align*}

Recordamos de la tarea anterior que el gradiente para Rosembrock de n variables está dado por:
\vspace{-1ex}
$$
 \begin{pmatrix}
  -400(x_2 - x_1^2)x_1 - 2(1 - x_1) \\
  200(x_2 - x_1^2) - 400(x_3 - x_2^2)x_2 - 2(1 - x_2)\\
  \vdots\\
  200(x_{i} - x_{i-1}^2) - 400(x_{i+1} - x_i^2)x_i - 2(1 - x_i)\\
  \vdots\\
  200(x_{n} - x_{n-1}^2)
 \end{pmatrix}
$$

\newpage
\item Función de Wood
\vspace{-1ex}
\begin{align*} 
f(\boldsymbol{x}) &= 100(x_1^2-x_2)^2 +(x_1-1)^2+(x_3-1)^2\\
&+90(x_3^2-x_4)^2+10.1[(x_2-1)^2+(x_4-1)^2]\\
&+19.8(x_2-1)(x_4-1) \\
\boldsymbol{x^0} &= [-3, -1, -3, -1]^T \\
\boldsymbol{x^*} &= [1, 1, 1, 1]^T \\
f(\boldsymbol{x^*}) &= 0
\end{align*}

\end{enumerate}

\end{enumerate}
\end{document}